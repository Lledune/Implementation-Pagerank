{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph manipulation - exercises\n",
    "## Introduction\n",
    "\n",
    "This is the third exercise notebook and its focus is on graph manipulation. The cells below contain the exercises. We have prepared parts of the code, you need to fill in the blanks. We suggest using package **networkx** to construct networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "#### Question 1.1\n",
    "PageRank is a method for ranking nodes. Explain why it is so popular and why it is useful.\n",
    "\n",
    "#### Question 1.2\n",
    "Implement iterative and exact algebraic versions of the PageRank algorithm. Use an **undirected** Barabasi-Albert random network on 1000 nodes and compare the answers using Mean Squared Error (L2 norm of the difference between true and estimated vectors). Use $\\alpha = 0.85$.\n",
    "\n",
    "#### Question 1.3\n",
    "What are the advantages of the iterative versus the exact methods for calculating PageRank?\n",
    "\n",
    "#### Question 1.4\n",
    "Using the iterative version, calculate the PageRank for a range of $\\alpha$ values between 0 and 1. How does the choice of *alpha* influence the results (the overall ranking and the number of iterations)? Why do you think $\\alpha = 0.85$ is a popular choice?\n",
    "\n",
    "#### Question 1.5\n",
    "List the problems that can arise when calculating the PageRank on **directed** graphs and explain how you might deal with these in practice. Generate a directed scale-free graph and show how the pageRank changes between the directed and undirected versions of the graph. Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse :  8.729242231093589e-08\n",
      "\n",
      " iterations : \n",
      "0.1 = 5\n",
      "0.3 = 7\n",
      "0.5 = 8\n",
      "0.85 = 10\n"
     ]
    }
   ],
   "source": [
    "# 1.1 #######################################\n",
    "# =============================================================================\n",
    "# 1.1\tPagerank is a very useful algorithm that is used to assign a rank to each nodes \n",
    "# by the use of a random surfer on the graph. We just have to look at how well Google \n",
    "# performed back when they first implemented it. To prevent the surfer to get stuck and \n",
    "# make it switch from node to node more smoothly we had a low probability (alpha/n) to \n",
    "# jump from current node to any other node with an uniform probability. \n",
    "# The algorithm is very useful for search engines where they assign a rank to each page to know \n",
    "# which pages should be shown first in the search. It also provides a fast algorithm for approximating the PageRank vector. \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# 1.2 #######################################\n",
    "#First is iterative method, then exact method\n",
    "\n",
    "import numpy as np \n",
    "import networkx as nx\n",
    "from scipy import linalg\n",
    "\n",
    "nNodes = 1000\n",
    "G = nx.barabasi_albert_graph(nNodes, 25, seed = 1234)\n",
    "#G is the generated graph\n",
    "#S is the matrix representing links\n",
    "def pageRankIt(G, alpha = 0.85, K = 2000):\n",
    "    nNodes = nx.number_of_nodes(G)\n",
    "\n",
    "    #Creating the network\n",
    "    S = nx.to_numpy_matrix(G) #Numpy adj matrix\n",
    "    \n",
    "    for i in range(0, nNodes): \n",
    "        S[i, :] = S[i, :] / np.sum(S[i, :])\n",
    "    summ = np.sum(S, axis = 1) #check == 1, probability that surfer goes from i to j\n",
    "    \n",
    "    v = np.random.rand(nNodes, 1) #initial guess\n",
    "    v = v / np.linalg.norm(v, 1) #L1\n",
    "    \n",
    "    #google matrix \n",
    "    GM = (alpha * S) + (((1 - alpha) / nNodes) * np.matmul(np.ones(nNodes), v))\n",
    "    summG = np.sum(GM, axis = 1) #check == 1, stochastic matrix\n",
    "    \n",
    "    for i in range(0, nNodes):\n",
    "        GM[i, :] = GM[i, :] / np.sum(GM[i, :])\n",
    "    \n",
    "    v = v.transpose()\n",
    "    \n",
    "    for i in range(0, K):\n",
    "        v = np.matmul(v, GM)\n",
    "        \n",
    "    v = v.transpose() #put it back in column\n",
    "    \n",
    "    #Normalize vector by its norm\n",
    "    v = v/np.linalg.norm(v, 1)\n",
    "\n",
    "    return v\n",
    "    \n",
    "\n",
    "    \n",
    "def pageRankEig(G, alpha = 0.85):\n",
    "#real pagerank vector ? = google matrix eigenvector \n",
    "    nNodes = nx.number_of_nodes(G)\n",
    "    Mat = nx.to_numpy_matrix(G) #Numpy adj matrix\n",
    "    \n",
    "    #S matrix\n",
    "    S = Mat\n",
    "    \n",
    "    for i in range(0, nNodes):\n",
    "        S[i, :] = S[i, :] / np.sum(S[i, :])\n",
    "    summ = np.sum(S, axis = 1) #check == 1, probability that surfer goes from i to j\n",
    "    \n",
    "    v = np.random.rand(nNodes, 1) #initial guess\n",
    "    v = v / np.linalg.norm(v, 1) #L1\n",
    "    \n",
    "    #google matrix \n",
    "    GM = (alpha * S) + (((1 - alpha) / nNodes) * np.matmul(np.ones(nNodes), v))\n",
    "    summG = np.sum(GM, axis = 1) #check == 1, stochastic matrix\n",
    "    \n",
    "    for i in range(0, nNodes):\n",
    "        GM[i, :] = GM[i, :] / np.sum(GM[i, :])\n",
    "    eigValue, eigVector = linalg.eigh(GM, check_finite = True)\n",
    "    eigV = eigVector[:,nNodes - 1]\n",
    "    eigV = eigV * -1 #positive pr\n",
    "    \n",
    "    #Normalize vector by its norm so it equals iteration method\n",
    "    eigV = eigV / np.linalg.norm(eigV, 1)\n",
    "    \n",
    "    return eigV\n",
    "    \n",
    "v = pageRankIt(G)\n",
    "eigV = pageRankEig(G)\n",
    "#print(v)\n",
    "#print(eigV)\n",
    "\n",
    "#Length must be = \n",
    "def MSE(eigV, v, N):\n",
    "    mseArr = np.zeros(N)\n",
    "    for i in range(0, nNodes):\n",
    "        mseArr[i] = (eigV[i] - v[i])**2\n",
    "    mse = np.sum(mseArr)/nNodes #8.729242231093588e-08\n",
    "    return mse\n",
    "\n",
    "mse = MSE(eigV, v, nNodes)\n",
    "print(\"mse : \", mse)\n",
    "\n",
    "# 1.2 ############################### Comments\n",
    "# The matrix we have used in eigenvector computation is of \"high\" dimension (not quite like the ones used by Google but still) and that \n",
    "# makes the spectral decomposition hard to compute because we have a lot of 0's in the matrix. Thus the values are not exactly the same (at least that is my interpretation of why they differ).\n",
    "# But overall, an mse of 0.00000008 seems small enough to say that both our implementations are very similar, even if the iteration method seems better suited for high dimension work.\n",
    "# Also, even tough our google matrix is stochastic (as we checked for it) is 1.41, not 1. The property says that the largest value should be 1, thus showing that the spectral decomposition wasn't \n",
    "# performed 100% accurately, however i did not find a way to compensate for this other than normalizing the vector by their L1 norm.\n",
    "\n",
    "# 1.3 ###############################\n",
    "# =============================================================================\n",
    "# The power method (iterative method) had some advantages, like the parameter alpha for convergence rate, \n",
    "# the convergence is independant of matrix dimension, it only stores in a single vector. It is also accurate (no substractions) \n",
    "# and very simple. \n",
    "# The matrix that will be dealt with will be immense because we are analyzing millions of pages, \n",
    "# thus we need an efficient way to calculate the eigenvector of a square matrix with very high dimension. \n",
    "# It is very effective because we do not have to compute a matrix decomposition which is nearly impossible for \n",
    "# high dimensionnal matrices with very few values. The downside is that we are only able to compute the largest eigenvector with \n",
    "# the iterative method. We can also say that having control on alpha is very good because it allows us to have control on the \n",
    "# speed of convergence. \n",
    "# =============================================================================\n",
    "\n",
    "# 1.4 ###############################    \n",
    "\n",
    "\n",
    "#Epsilon is the threshold\n",
    "def pageRankItEpsilon(G, alpha = 0.85, eps = 0.00000001):\n",
    "    nNodes = nx.number_of_nodes(G)\n",
    "    iterationsCounter = 0\n",
    "    \n",
    "    #Creating the network\n",
    "    S = nx.to_numpy_matrix(G) #Numpy adj matrix\n",
    "    \n",
    "    for i in range(0, nNodes): \n",
    "        S[i, :] = S[i, :] / np.sum(S[i, :])\n",
    "    summ = np.sum(S, axis = 1) #check == 1, probability that surfer goes from i to j\n",
    "    \n",
    "    v = np.random.rand(nNodes, 1) #initial guess\n",
    "    v = v / np.linalg.norm(v, 1) #L1\n",
    "    last_v = np.ones((nNodes, 1), dtype=np.float32) * 1000 #initial pass for epsilon \n",
    "    #google matrix \n",
    "    GM = (alpha * S) + (((1 - alpha) / nNodes) * np.matmul(np.ones(nNodes), v))\n",
    "    summG = np.sum(GM, axis = 1) #check == 1, stochastic matrix        \n",
    "    \n",
    "    for i in range(0, nNodes):\n",
    "        GM[i, :] = GM[i, :] / np.sum(GM[i, :])\n",
    "        \n",
    "    v = v.transpose()\n",
    "    \n",
    "    while np.linalg.norm(v-last_v, 2) > eps:\n",
    "        iterationsCounter += 1\n",
    "        last_v = v\n",
    "        v = np.matmul(v, GM)\n",
    "        \n",
    "    v = v.transpose() #put it back in column\n",
    "    \n",
    "    #Normalize vector by its norm\n",
    "    v = v/np.linalg.norm(v, 1)\n",
    "    \n",
    "    #Prepare list for return \n",
    "    ret = [v, iterationsCounter] \n",
    "    \n",
    "    return ret\n",
    "\n",
    "alphaList = [0.1,0.3,0.5,0.85]\n",
    "mseAlpha = []\n",
    "\n",
    "for alpha in alphaList:\n",
    "    mseAlpha.append(pageRankItEpsilon(G, alpha))\n",
    "print(\"\\n iterations : \")\n",
    "\n",
    "for i in range(0,4):\n",
    "    print(alphaList[i], \"=\", mseAlpha[i][1])\n",
    "    \n",
    "#Checking the results, it seems that the order of the pages wont change much but some things are noticeable here. \n",
    "#First, the values tend to be bigger as alpha grows. we also get \n",
    "#Second, the gap between low values and higher values is bigger, this could help with the ranking.\n",
    "#Third, the number of iterations go up as alpha grows so it would mean more time to process but it seemed reasonable as the iterations went from 5 to 10 (0.1 vs 0.85). \n",
    "#0.85 is probably a popular choice because of these reasons. \n",
    "#Alpha/n gives a smoother walk as we add the teleportation\n",
    "    \n",
    "\n",
    "# 1.5 #################################\n",
    "# Spectral theory is different, asymmetric matrix may not be diagonalizable.\n",
    "# Eigenvectors may not be orthogonal\n",
    "# probability of vertices can vary\n",
    "# Very slow to converge to distribution \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "This exercise aims to introduce you to one of the most important topics of networks science: community detection.\n",
    "Network communities are groups of nodes that share a similar pattern of connectivity.\n",
    "\n",
    "One way to generate synthetically random networks with a well defined community structure is with the stochactic block model (SBM). The SBM is a probabilistic generative model which assigns a probability for generating an edge between two nodes i,j in a network. \n",
    "\n",
    "Like other generative models (e.g., LDA), the SBM defines a probability distribution over networks $ P(G|\\theta) $, where $\\theta$ is the set of parameters characterising the edge probabilities under the model. Thus, given a choice of $\\theta$ we can draw or generate a network instance $G$ from the distribution $P(G|\\theta)$.\n",
    "\n",
    "#### Model definition\n",
    "The standard version of SBM can be defined with the following variables $\\theta = (k,z,M)$:\n",
    "* $k$: an positive integer value defining the number of groups of communities in the network\n",
    "* $z$: a $1 \\times N$ vector, where $z(l)$ gives the group index of vertex $l$\n",
    "* $M$: a $k \\times k$ stochastic matrix, where $M(i,j)$ gives the probability that a vertex of group $i$ is conected to a vertex of group $j$.\n",
    "\n",
    "and $N$ the number of nodes of the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "A simplified version of the SBM is where the entries in the stochastic matrix $M$ have only two possible values $M_{r,r} = p_{\\rm{in}}$ and $M_{r,s} = p_{\\rm{out}}$ (for $r \\neq s$).\n",
    "\n",
    "Use the SBM formulation introduced above to generate a random network of 50 nodes made up of three weakly connected $(p_{\\rm{out}} = 0.001)$ cliques $(p_{\\rm{in}} = 1)$ as communities of 10,15 and 25 nodes respectively.\n",
    "\n",
    "Visualise the adjacency matrix using `imshow` from matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "Write a function __create_graph_k_cliques(k)__ which generates a network with k *weakly connected* cliques of size 10.\n",
    "\n",
    "Use the function to generate a graph with $k=10$ cliques and plot the adjacency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "Use the previous function to generate networks with $k \\in \\{2,3,..,150 \\}$ cliques of size 10. \n",
    "\n",
    "Apply the Louvain method to each of the networks. What do you notice? \n",
    "\n",
    "(We suggest you use the `python-louvain` package)\n",
    "\n",
    "Calculate the number of communities retrieved by Louvain algorithm and compare them with the true number of communities generated by the SBM model. \n",
    "\n",
    "What do you observe? Can you explain why this happens?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
